{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cuNxae1QB0w7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h5F5vDGyiDrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a286d77-6a59-4e81-c268-1aeda5ef3d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fDLkF7LFiHGD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jq5n9KLOiHW4"
      },
      "outputs": [],
      "source": [
        "# Importing Data\n",
        "train=pd.read_csv('/content/drive/MyDrive/datasets/train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-JNP8abNiJHm"
      },
      "outputs": [],
      "source": [
        "test=pd.read_csv('/content/drive/MyDrive/datasets/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pbEcBHebiYLA"
      },
      "outputs": [],
      "source": [
        "meal=pd.read_csv('/content/drive/MyDrive/datasets/meal_info.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mYBSY2C6iZuW"
      },
      "outputs": [],
      "source": [
        "# Merging DataFrames\n",
        "train=train.merge(meal, on='meal_id')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7M6AfnfOjX1b"
      },
      "outputs": [],
      "source": [
        "# Take inputs from the user\n",
        "center_id = 55\n",
        "meal_id = 1993"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FztGSO2pjcj1"
      },
      "outputs": [],
      "source": [
        "# Filter training data\n",
        "train_df = train[train['center_id']==center_id]\n",
        "train_df = train_df[train_df['meal_id']==meal_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Cuzwu_VSjen8"
      },
      "outputs": [],
      "source": [
        "# Preprocessing for time-series analysis\n",
        "def pretime():\n",
        "\n",
        "  df=train_df\n",
        "  period = len(train_df)\n",
        "  train_df['Date'] = pd.date_range('2015-01-08', periods=period, freq='W')\n",
        "  train_df['Day'] = train_df['Date'].dt.day\n",
        "  train_df['Month'] = train_df['Date'].dt.month\n",
        "  train_df['Year'] = train_df['Date'].dt.year\n",
        "  train_df['Quarter'] = train_df['Date'].dt.quarter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DckoI1HTjge2"
      },
      "outputs": [],
      "source": [
        "pretime()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "TP75ty7IuGK-",
        "outputId": "835a4465-13de-4902-c118-4a4c0be30686"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            id  week  center_id  meal_id  checkout_price  base_price  \\\n",
              "11092  1466964     1         55     1993          136.83      135.83   \n",
              "11169  1035555     2         55     1993          133.89      133.89   \n",
              "11246  1282652     3         55     1993          134.86      133.86   \n",
              "11323  1066744     4         55     1993          134.89      133.89   \n",
              "11400  1115779     5         55     1993          147.50      145.50   \n",
              "...        ...   ...        ...      ...             ...         ...   \n",
              "21823  1210404   141         55     1993          149.41      148.41   \n",
              "21900  1162047   142         55     1993          152.35      152.35   \n",
              "21977  1006104   143         55     1993          151.35      150.35   \n",
              "22054  1107848   144         55     1993          151.35      150.35   \n",
              "22131  1401715   145         55     1993          159.14      158.14   \n",
              "\n",
              "       emailer_for_promotion  homepage_featured  num_orders   category  \\\n",
              "11092                      0                  0         270  Beverages   \n",
              "11169                      0                  0         121  Beverages   \n",
              "11246                      0                  0         258  Beverages   \n",
              "11323                      0                  0          82  Beverages   \n",
              "11400                      0                  0          81  Beverages   \n",
              "...                      ...                ...         ...        ...   \n",
              "21823                      0                  0         134  Beverages   \n",
              "21900                      0                  0         189  Beverages   \n",
              "21977                      0                  0         109  Beverages   \n",
              "22054                      0                  0         190  Beverages   \n",
              "22131                      0                  0          54  Beverages   \n",
              "\n",
              "      cuisine       Date  Day  Month  Year  Quarter  \n",
              "11092    Thai 2015-01-11   11      1  2015        1  \n",
              "11169    Thai 2015-01-18   18      1  2015        1  \n",
              "11246    Thai 2015-01-25   25      1  2015        1  \n",
              "11323    Thai 2015-02-01    1      2  2015        1  \n",
              "11400    Thai 2015-02-08    8      2  2015        1  \n",
              "...       ...        ...  ...    ...   ...      ...  \n",
              "21823    Thai 2017-09-17   17      9  2017        3  \n",
              "21900    Thai 2017-09-24   24      9  2017        3  \n",
              "21977    Thai 2017-10-01    1     10  2017        4  \n",
              "22054    Thai 2017-10-08    8     10  2017        4  \n",
              "22131    Thai 2017-10-15   15     10  2017        4  \n",
              "\n",
              "[145 rows x 16 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a943750-b05b-421c-9bdc-a326ea328de8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>week</th>\n",
              "      <th>center_id</th>\n",
              "      <th>meal_id</th>\n",
              "      <th>checkout_price</th>\n",
              "      <th>base_price</th>\n",
              "      <th>emailer_for_promotion</th>\n",
              "      <th>homepage_featured</th>\n",
              "      <th>num_orders</th>\n",
              "      <th>category</th>\n",
              "      <th>cuisine</th>\n",
              "      <th>Date</th>\n",
              "      <th>Day</th>\n",
              "      <th>Month</th>\n",
              "      <th>Year</th>\n",
              "      <th>Quarter</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11092</th>\n",
              "      <td>1466964</td>\n",
              "      <td>1</td>\n",
              "      <td>55</td>\n",
              "      <td>1993</td>\n",
              "      <td>136.83</td>\n",
              "      <td>135.83</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>270</td>\n",
              "      <td>Beverages</td>\n",
              "      <td>Thai</td>\n",
              "      <td>2015-01-11</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11169</th>\n",
              "      <td>1035555</td>\n",
              "      <td>2</td>\n",
              "      <td>55</td>\n",
              "      <td>1993</td>\n",
              "      <td>133.89</td>\n",
              "      <td>133.89</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>121</td>\n",
              "      <td>Beverages</td>\n",
              "      <td>Thai</td>\n",
              "      <td>2015-01-18</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11246</th>\n",
              "      <td>1282652</td>\n",
              "      <td>3</td>\n",
              "      <td>55</td>\n",
              "      <td>1993</td>\n",
              "      <td>134.86</td>\n",
              "      <td>133.86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>258</td>\n",
              "      <td>Beverages</td>\n",
              "      <td>Thai</td>\n",
              "      <td>2015-01-25</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11323</th>\n",
              "      <td>1066744</td>\n",
              "      <td>4</td>\n",
              "      <td>55</td>\n",
              "      <td>1993</td>\n",
              "      <td>134.89</td>\n",
              "      <td>133.89</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "      <td>Beverages</td>\n",
              "      <td>Thai</td>\n",
              "      <td>2015-02-01</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11400</th>\n",
              "      <td>1115779</td>\n",
              "      <td>5</td>\n",
              "      <td>55</td>\n",
              "      <td>1993</td>\n",
              "      <td>147.50</td>\n",
              "      <td>145.50</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>81</td>\n",
              "      <td>Beverages</td>\n",
              "      <td>Thai</td>\n",
              "      <td>2015-02-08</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21823</th>\n",
              "      <td>1210404</td>\n",
              "      <td>141</td>\n",
              "      <td>55</td>\n",
              "      <td>1993</td>\n",
              "      <td>149.41</td>\n",
              "      <td>148.41</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>134</td>\n",
              "      <td>Beverages</td>\n",
              "      <td>Thai</td>\n",
              "      <td>2017-09-17</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>2017</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21900</th>\n",
              "      <td>1162047</td>\n",
              "      <td>142</td>\n",
              "      <td>55</td>\n",
              "      <td>1993</td>\n",
              "      <td>152.35</td>\n",
              "      <td>152.35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>189</td>\n",
              "      <td>Beverages</td>\n",
              "      <td>Thai</td>\n",
              "      <td>2017-09-24</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>2017</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21977</th>\n",
              "      <td>1006104</td>\n",
              "      <td>143</td>\n",
              "      <td>55</td>\n",
              "      <td>1993</td>\n",
              "      <td>151.35</td>\n",
              "      <td>150.35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>109</td>\n",
              "      <td>Beverages</td>\n",
              "      <td>Thai</td>\n",
              "      <td>2017-10-01</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>2017</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22054</th>\n",
              "      <td>1107848</td>\n",
              "      <td>144</td>\n",
              "      <td>55</td>\n",
              "      <td>1993</td>\n",
              "      <td>151.35</td>\n",
              "      <td>150.35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>190</td>\n",
              "      <td>Beverages</td>\n",
              "      <td>Thai</td>\n",
              "      <td>2017-10-08</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>2017</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22131</th>\n",
              "      <td>1401715</td>\n",
              "      <td>145</td>\n",
              "      <td>55</td>\n",
              "      <td>1993</td>\n",
              "      <td>159.14</td>\n",
              "      <td>158.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>54</td>\n",
              "      <td>Beverages</td>\n",
              "      <td>Thai</td>\n",
              "      <td>2017-10-15</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>2017</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>145 rows × 16 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a943750-b05b-421c-9bdc-a326ea328de8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a943750-b05b-421c-9bdc-a326ea328de8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a943750-b05b-421c-9bdc-a326ea328de8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bf773b0d-1338-4c69-ba91-a4ee625d6e9e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bf773b0d-1338-4c69-ba91-a4ee625d6e9e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bf773b0d-1338-4c69-ba91-a4ee625d6e9e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "ThHrWTOBZ6xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_data = train_df.drop(columns=['id','center_id','meal_id','category','cuisine'])\n",
        "lstm_data = lstm_data.set_index(['Date'])"
      ],
      "metadata": {
        "id": "_SA3oE_hZ8uy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "period=len(train_df)"
      ],
      "metadata": {
        "id": "r3Gchhz-aAwD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = lstm_data.drop(columns='num_orders')\n",
        "y_train = lstm_data['num_orders']\n",
        "y_train = np.log1p(y_train)\n",
        "split_size = period-15\n",
        "X_train = x_train.iloc[:split_size,:]\n",
        "X_test = x_train.iloc[split_size:,:]\n",
        "Y_train = y_train.iloc[:split_size]\n",
        "Y_test = y_train.iloc[split_size:]"
      ],
      "metadata": {
        "id": "3wIaw0aXaBpL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_data"
      ],
      "metadata": {
        "id": "N4qvRo2faEQv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "cc4dfda7-27af-4a50-df5d-03ec5c1e228e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            week  checkout_price  base_price  emailer_for_promotion  \\\n",
              "Date                                                                  \n",
              "2015-01-11     1          136.83      135.83                      0   \n",
              "2015-01-18     2          133.89      133.89                      0   \n",
              "2015-01-25     3          134.86      133.86                      0   \n",
              "2015-02-01     4          134.89      133.89                      0   \n",
              "2015-02-08     5          147.50      145.50                      0   \n",
              "...          ...             ...         ...                    ...   \n",
              "2017-09-17   141          149.41      148.41                      0   \n",
              "2017-09-24   142          152.35      152.35                      0   \n",
              "2017-10-01   143          151.35      150.35                      0   \n",
              "2017-10-08   144          151.35      150.35                      0   \n",
              "2017-10-15   145          159.14      158.14                      0   \n",
              "\n",
              "            homepage_featured  num_orders  Day  Month  Year  Quarter  \n",
              "Date                                                                  \n",
              "2015-01-11                  0         270   11      1  2015        1  \n",
              "2015-01-18                  0         121   18      1  2015        1  \n",
              "2015-01-25                  0         258   25      1  2015        1  \n",
              "2015-02-01                  0          82    1      2  2015        1  \n",
              "2015-02-08                  0          81    8      2  2015        1  \n",
              "...                       ...         ...  ...    ...   ...      ...  \n",
              "2017-09-17                  0         134   17      9  2017        3  \n",
              "2017-09-24                  0         189   24      9  2017        3  \n",
              "2017-10-01                  0         109    1     10  2017        4  \n",
              "2017-10-08                  0         190    8     10  2017        4  \n",
              "2017-10-15                  0          54   15     10  2017        4  \n",
              "\n",
              "[145 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49061978-2e00-4f62-9e2f-6a63fe8cdd43\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>week</th>\n",
              "      <th>checkout_price</th>\n",
              "      <th>base_price</th>\n",
              "      <th>emailer_for_promotion</th>\n",
              "      <th>homepage_featured</th>\n",
              "      <th>num_orders</th>\n",
              "      <th>Day</th>\n",
              "      <th>Month</th>\n",
              "      <th>Year</th>\n",
              "      <th>Quarter</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-01-11</th>\n",
              "      <td>1</td>\n",
              "      <td>136.83</td>\n",
              "      <td>135.83</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>270</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-18</th>\n",
              "      <td>2</td>\n",
              "      <td>133.89</td>\n",
              "      <td>133.89</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>121</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-25</th>\n",
              "      <td>3</td>\n",
              "      <td>134.86</td>\n",
              "      <td>133.86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>258</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-02-01</th>\n",
              "      <td>4</td>\n",
              "      <td>134.89</td>\n",
              "      <td>133.89</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-02-08</th>\n",
              "      <td>5</td>\n",
              "      <td>147.50</td>\n",
              "      <td>145.50</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>81</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-09-17</th>\n",
              "      <td>141</td>\n",
              "      <td>149.41</td>\n",
              "      <td>148.41</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>134</td>\n",
              "      <td>17</td>\n",
              "      <td>9</td>\n",
              "      <td>2017</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-09-24</th>\n",
              "      <td>142</td>\n",
              "      <td>152.35</td>\n",
              "      <td>152.35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>189</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>2017</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-10-01</th>\n",
              "      <td>143</td>\n",
              "      <td>151.35</td>\n",
              "      <td>150.35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>109</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>2017</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-10-08</th>\n",
              "      <td>144</td>\n",
              "      <td>151.35</td>\n",
              "      <td>150.35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>190</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>2017</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-10-15</th>\n",
              "      <td>145</td>\n",
              "      <td>159.14</td>\n",
              "      <td>158.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>54</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>2017</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>145 rows × 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49061978-2e00-4f62-9e2f-6a63fe8cdd43')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-49061978-2e00-4f62-9e2f-6a63fe8cdd43 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-49061978-2e00-4f62-9e2f-6a63fe8cdd43');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-aa7534da-ad5d-4a2c-ad0f-ae6f2a318399\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aa7534da-ad5d-4a2c-ad0f-ae6f2a318399')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-aa7534da-ad5d-4a2c-ad0f-ae6f2a318399 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM (Light Model)"
      ],
      "metadata": {
        "id": "in95tB5Zckji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Columns to train on\n",
        "columns_to_train = [\"week\", \"checkout_price\", \"base_price\", \"emailer_for_promotion\", \"homepage_featured\", \"Day\", \"Month\", \"Year\", \"Quarter\"]\n",
        "\n",
        "# Extract features and target variable\n",
        "X = lstm_data[columns_to_train].values\n",
        "y = lstm_data[\"num_orders\"].values\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LSTM input (assuming a time series structure)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compilation\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(loss='mse', optimizer=optimizer)\n",
        "\n",
        "# Training\n",
        "history = model.fit(x=X_train, y=y_train, epochs=200, batch_size=512, validation_data=(X_test, y_test))\n",
        "\n",
        "# Print MSE\n",
        "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Training MSE: {history.history[\"loss\"][-1]}, Test MSE: {test_loss}')\n"
      ],
      "metadata": {
        "id": "6D5de9TJa5QQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e6b363-db90-42cc-83b8-7ee1964ca445"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 82222.5625 - val_loss: 67146.5000\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 82219.8594 - val_loss: 67143.7812\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82216.8594 - val_loss: 67141.0547\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82214.1094 - val_loss: 67138.3516\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82211.1797 - val_loss: 67135.6406\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82208.9453 - val_loss: 67132.9297\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82205.5938 - val_loss: 67130.1953\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82202.8359 - val_loss: 67127.3906\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 82200.3672 - val_loss: 67124.5156\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82196.8281 - val_loss: 67121.5469\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82194.1953 - val_loss: 67118.4531\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 82190.2578 - val_loss: 67115.2422\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82187.0078 - val_loss: 67111.8906\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82184.3516 - val_loss: 67108.4219\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82180.6562 - val_loss: 67104.8203\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82176.5000 - val_loss: 67101.0703\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82172.3828 - val_loss: 67097.1172\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82168.0547 - val_loss: 67092.9609\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82163.9766 - val_loss: 67088.5547\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82158.0625 - val_loss: 67083.8594\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82154.1094 - val_loss: 67078.8750\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82150.0547 - val_loss: 67073.5938\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82142.8359 - val_loss: 67067.9922\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82137.7344 - val_loss: 67062.0469\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82135.6562 - val_loss: 67055.7344\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82125.9375 - val_loss: 67049.0000\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82121.1172 - val_loss: 67041.8672\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82110.0000 - val_loss: 67034.2344\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82100.2422 - val_loss: 67026.0547\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 82098.2734 - val_loss: 67017.3047\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82085.5000 - val_loss: 67007.9062\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 82076.6562 - val_loss: 66997.7891\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82060.5078 - val_loss: 66986.8516\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 82056.0156 - val_loss: 66975.0547\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82052.5781 - val_loss: 66962.4219\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 82034.0781 - val_loss: 66948.7578\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 82023.5156 - val_loss: 66934.0469\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81996.7422 - val_loss: 66918.0703\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81981.0156 - val_loss: 66900.7188\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 81974.0938 - val_loss: 66881.9219\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81936.3594 - val_loss: 66861.4453\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81933.1641 - val_loss: 66839.2734\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81909.0234 - val_loss: 66815.2578\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81875.7344 - val_loss: 66789.1719\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81851.1641 - val_loss: 66760.8125\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81836.4844 - val_loss: 66730.1562\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81819.2500 - val_loss: 66697.0781\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 81752.6172 - val_loss: 66661.1016\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81712.5781 - val_loss: 66622.1328\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81711.3594 - val_loss: 66580.2578\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81624.9062 - val_loss: 66534.9453\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81587.9453 - val_loss: 66486.0312\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 81563.4766 - val_loss: 66433.4844\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81490.8672 - val_loss: 66377.0703\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81450.9141 - val_loss: 66316.6016\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 81373.3203 - val_loss: 66252.0391\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 81294.3438 - val_loss: 66183.1250\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 81254.8984 - val_loss: 66110.0000\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 81226.2969 - val_loss: 66032.7500\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 81062.4844 - val_loss: 65950.7578\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 81065.8984 - val_loss: 65864.4062\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 80967.5938 - val_loss: 65773.3906\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 80823.1094 - val_loss: 65677.5859\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 80707.8828 - val_loss: 65576.7266\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 80680.4453 - val_loss: 65470.9297\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 80523.7578 - val_loss: 65359.9219\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 80401.7031 - val_loss: 65243.3711\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 80209.3984 - val_loss: 65120.9023\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 80211.8828 - val_loss: 64993.0781\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 79980.2578 - val_loss: 64859.4219\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 79893.0156 - val_loss: 64719.9727\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 79785.3359 - val_loss: 64574.6094\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 79448.9297 - val_loss: 64422.5273\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 79443.7500 - val_loss: 64264.3438\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 79051.6094 - val_loss: 64099.0898\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 79085.7578 - val_loss: 63927.4570\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 78821.6172 - val_loss: 63749.3242\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 78626.7422 - val_loss: 63564.3789\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 78354.8594 - val_loss: 63372.1094\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 78189.7734 - val_loss: 63172.5234\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 78079.9688 - val_loss: 62966.3164\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 77735.9297 - val_loss: 62752.8867\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 77472.1719 - val_loss: 62531.8906\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 77278.6172 - val_loss: 62303.5938\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 76915.3984 - val_loss: 62067.5000\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 76580.0859 - val_loss: 61823.3867\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 76431.7266 - val_loss: 61571.7578\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 76071.9453 - val_loss: 61312.1641\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 75930.5938 - val_loss: 61045.1133\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 75445.7031 - val_loss: 60770.2578\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 75343.4375 - val_loss: 60487.7656\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 74923.9844 - val_loss: 60197.4414\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 74537.2031 - val_loss: 59899.1211\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 74070.4766 - val_loss: 59592.3047\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 73900.5078 - val_loss: 59277.6133\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 73561.3984 - val_loss: 58955.2773\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 73261.2266 - val_loss: 58625.3516\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 72811.1875 - val_loss: 58287.5117\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 72533.8828 - val_loss: 57942.5938\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 72175.6250 - val_loss: 57590.4727\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 71606.7500 - val_loss: 57230.9219\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 71152.9453 - val_loss: 56863.9492\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 70593.4766 - val_loss: 56489.2266\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 70442.8047 - val_loss: 56108.0078\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 69781.8750 - val_loss: 55719.5352\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 69516.2812 - val_loss: 55324.4414\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 69314.8594 - val_loss: 54923.6367\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 68121.3906 - val_loss: 54515.2656\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 68021.6719 - val_loss: 54100.8750\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 67330.0938 - val_loss: 53680.3203\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 66569.6562 - val_loss: 53253.0742\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 66230.0547 - val_loss: 52819.9258\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 65791.3750 - val_loss: 52381.6133\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 64956.5312 - val_loss: 51937.7344\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 64305.4766 - val_loss: 51488.5430\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 64554.0703 - val_loss: 51035.9609\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 63550.1133 - val_loss: 50579.2070\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 62820.1211 - val_loss: 50118.4141\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 62696.4141 - val_loss: 49654.7617\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 61726.0586 - val_loss: 49188.0078\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 61258.7500 - val_loss: 48718.7422\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 60874.8789 - val_loss: 48247.5352\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 59972.4062 - val_loss: 47774.4688\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 59931.8008 - val_loss: 47300.6914\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 58834.0859 - val_loss: 46825.8438\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 58597.4219 - val_loss: 46351.0078\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 57956.5547 - val_loss: 45876.3008\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 56504.3203 - val_loss: 45401.1484\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 56794.6523 - val_loss: 44926.9453\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 56009.0781 - val_loss: 44454.4688\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 55416.8086 - val_loss: 43983.9453\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 54220.8203 - val_loss: 43515.6172\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 53392.7070 - val_loss: 43049.6914\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 52959.1211 - val_loss: 42586.8281\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 52559.2422 - val_loss: 42127.8320\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 52241.3516 - val_loss: 41673.5156\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 51110.5156 - val_loss: 41224.2773\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 50468.2773 - val_loss: 40780.3789\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 48926.7031 - val_loss: 40341.9297\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 50030.9922 - val_loss: 39910.4531\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 48688.8984 - val_loss: 39485.8711\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 48058.3828 - val_loss: 39068.5859\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 48010.6289 - val_loss: 38659.5312\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 46110.6562 - val_loss: 38258.8438\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 45333.5742 - val_loss: 37866.8477\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 45148.4648 - val_loss: 37484.2500\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 44747.6406 - val_loss: 37111.3008\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 44107.0352 - val_loss: 36748.5469\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 43760.0078 - val_loss: 36396.3281\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 43182.4609 - val_loss: 36055.0586\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 42935.0039 - val_loss: 35725.3047\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 42048.8086 - val_loss: 35407.3945\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 40963.9961 - val_loss: 35101.5312\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 41015.9297 - val_loss: 34807.8906\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 41028.3672 - val_loss: 34526.4844\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 40434.2070 - val_loss: 34257.8789\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 37978.6484 - val_loss: 34001.8789\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 39885.3359 - val_loss: 33759.0781\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 38836.9453 - val_loss: 33528.8477\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 36879.5234 - val_loss: 33311.5898\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 35954.4648 - val_loss: 33107.4258\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 36774.2773 - val_loss: 32916.1172\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 35955.2969 - val_loss: 32737.6680\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 36370.6094 - val_loss: 32572.1992\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 34273.1914 - val_loss: 32419.5566\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 33842.6445 - val_loss: 32279.4805\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 34291.5117 - val_loss: 32151.6680\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 33364.0781 - val_loss: 32036.0918\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 32423.1914 - val_loss: 31931.8613\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 31939.8926 - val_loss: 31838.3789\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 31907.5840 - val_loss: 31756.0664\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 31514.9160 - val_loss: 31684.9004\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 31476.6953 - val_loss: 31624.3926\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 31835.3984 - val_loss: 31574.4023\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 30717.0996 - val_loss: 31533.8184\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 30742.5449 - val_loss: 31503.4219\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 30077.6387 - val_loss: 31481.2207\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 29500.7930 - val_loss: 31467.1367\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 28839.8672 - val_loss: 31460.2930\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 29162.7578 - val_loss: 31460.0254\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 29694.3301 - val_loss: 31467.7930\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 28572.9766 - val_loss: 31482.4102\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 28691.9453 - val_loss: 31503.6719\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 27641.4258 - val_loss: 31530.5918\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 27762.6934 - val_loss: 31560.5137\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 26795.3301 - val_loss: 31594.3125\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 27393.1719 - val_loss: 31633.6660\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 26570.1016 - val_loss: 31676.9648\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 26404.9766 - val_loss: 31724.4004\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 26008.6836 - val_loss: 31776.2109\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 25849.5488 - val_loss: 31829.6211\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 25731.5898 - val_loss: 31886.1914\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 25450.8574 - val_loss: 31946.2148\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 24786.0957 - val_loss: 32004.3789\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 23785.8613 - val_loss: 32059.3789\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 24427.5996 - val_loss: 32116.1406\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 24423.2715 - val_loss: 32172.8008\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 24025.3535 - val_loss: 32237.0000\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 23999.5020 - val_loss: 32300.7090\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 23720.2754 - val_loss: 32366.3066\n",
            "Training MSE: 23720.275390625, Test MSE: 32366.306640625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bidirectional LSTM (Medium Model)"
      ],
      "metadata": {
        "id": "mAHlEhmkcp4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Columns to train on\n",
        "columns_to_train = [\"week\", \"checkout_price\", \"base_price\", \"emailer_for_promotion\", \"homepage_featured\", \"Day\", \"Month\", \"Year\", \"Quarter\"]\n",
        "\n",
        "# Extract features and target variable\n",
        "X = lstm_data[columns_to_train].values\n",
        "y = lstm_data[\"num_orders\"].values\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LSTM input (assuming a time series structure)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Model\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu', return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(50, activation='relu')))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compilation\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(loss='mse', optimizer=optimizer)\n",
        "\n",
        "# Training\n",
        "history = model.fit(x=X_train, y=y_train, epochs=200, batch_size=512, validation_data=(X_test, y_test))\n",
        "\n",
        "# Print MSE\n",
        "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Training MSE: {history.history[\"loss\"][-1]}, Test MSE: {test_loss}')\n"
      ],
      "metadata": {
        "id": "pSvmcAbLaHp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16967bf5-3062-4c98-da8f-b1e824c971d0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 5s 5s/step - loss: 82214.3906 - val_loss: 67136.5547\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 82210.2266 - val_loss: 67131.6641\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 82205.0469 - val_loss: 67126.6094\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 82200.2266 - val_loss: 67121.4375\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 82193.0859 - val_loss: 67115.9531\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 82186.2422 - val_loss: 67110.1406\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 82183.8594 - val_loss: 67104.0078\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 82177.1406 - val_loss: 67097.5156\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 82167.5547 - val_loss: 67090.5781\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 82159.8906 - val_loss: 67083.0781\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 82150.4219 - val_loss: 67074.9453\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 82144.3125 - val_loss: 67066.1172\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 82130.8594 - val_loss: 67056.5703\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 82122.1094 - val_loss: 67046.2422\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 82112.5312 - val_loss: 67035.0547\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 82100.6875 - val_loss: 67022.9062\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 82086.8906 - val_loss: 67009.6562\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 82078.9844 - val_loss: 66995.2734\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 82060.4688 - val_loss: 66979.6016\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 82046.0234 - val_loss: 66962.5234\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 82033.3281 - val_loss: 66943.9062\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 82013.8125 - val_loss: 66923.5547\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 81990.9766 - val_loss: 66901.2656\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 81969.7266 - val_loss: 66876.7734\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 81942.7734 - val_loss: 66849.8906\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 81911.9297 - val_loss: 66820.3047\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 81893.3359 - val_loss: 66787.7578\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 81860.0312 - val_loss: 66751.9844\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 81825.9453 - val_loss: 66712.6328\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81786.4844 - val_loss: 66669.3125\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81764.0156 - val_loss: 66621.7734\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81690.9766 - val_loss: 66569.5156\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 81667.5000 - val_loss: 66512.2969\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81567.0625 - val_loss: 66449.4375\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 81561.9766 - val_loss: 66380.9297\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81434.1719 - val_loss: 66305.9297\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 81349.6641 - val_loss: 66223.9453\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81317.4141 - val_loss: 66135.0547\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 81244.1016 - val_loss: 66038.8672\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 81131.3828 - val_loss: 65934.9219\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 81027.6875 - val_loss: 65822.9062\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 80884.9453 - val_loss: 65702.5469\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 80807.8594 - val_loss: 65573.7734\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 80660.0000 - val_loss: 65436.0703\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 80590.8828 - val_loss: 65289.3477\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 80397.9688 - val_loss: 65133.2852\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 80308.3828 - val_loss: 64967.8711\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 79924.1562 - val_loss: 64791.6836\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 79850.0781 - val_loss: 64604.6797\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 79685.5703 - val_loss: 64406.8203\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 79451.8984 - val_loss: 64197.6719\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 79404.4844 - val_loss: 63977.4258\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 79158.8828 - val_loss: 63745.4297\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 78925.7812 - val_loss: 63501.2500\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 78487.6641 - val_loss: 63243.4062\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 78542.9766 - val_loss: 62973.0352\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 77784.6016 - val_loss: 62687.9570\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 77692.1172 - val_loss: 62388.5664\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 77240.9062 - val_loss: 62074.2227\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 76909.6016 - val_loss: 61744.4570\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 76553.5625 - val_loss: 61399.3320\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 76289.6719 - val_loss: 61038.9336\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 76022.8984 - val_loss: 60663.7422\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 75521.7578 - val_loss: 60273.0078\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 75025.5312 - val_loss: 59866.1094\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 74656.6562 - val_loss: 59443.1211\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 73916.4141 - val_loss: 59003.1797\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 73121.1719 - val_loss: 58545.6914\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 72695.8594 - val_loss: 58071.2109\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 72600.9375 - val_loss: 57581.1797\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 72182.0000 - val_loss: 57076.2773\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 70943.2031 - val_loss: 56554.6836\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 70707.3594 - val_loss: 56017.9492\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 70338.3438 - val_loss: 55466.8281\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 69070.2422 - val_loss: 54899.8516\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 68532.4297 - val_loss: 54318.2773\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 67934.5312 - val_loss: 53722.7148\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 67621.9766 - val_loss: 53114.7656\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 66495.0938 - val_loss: 52493.8438\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 65259.9492 - val_loss: 51859.9648\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 65136.8438 - val_loss: 51215.6016\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 64529.4922 - val_loss: 50562.1641\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 63426.2656 - val_loss: 49899.6562\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 62609.1250 - val_loss: 49229.5781\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 61646.0703 - val_loss: 48552.5430\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 61106.6484 - val_loss: 47870.5977\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 60194.8633 - val_loss: 47184.7148\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 59529.2109 - val_loss: 46496.7578\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 57535.7422 - val_loss: 45807.0000\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 56868.0469 - val_loss: 45118.1406\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 57110.6406 - val_loss: 44433.5078\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 55676.2344 - val_loss: 43753.7734\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 54053.6914 - val_loss: 43080.3203\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 52638.9531 - val_loss: 42415.1211\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 52384.5586 - val_loss: 41761.0273\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 52304.7227 - val_loss: 41120.3789\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 51231.5898 - val_loss: 40495.5273\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 50204.3281 - val_loss: 39887.9023\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 48060.6758 - val_loss: 39299.6719\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 48536.8633 - val_loss: 38734.0586\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 46602.4180 - val_loss: 38193.0156\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 45181.7656 - val_loss: 37678.6094\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 43539.9258 - val_loss: 37192.2617\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 43767.3086 - val_loss: 36736.4102\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 41706.3477 - val_loss: 36313.6328\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 41980.2461 - val_loss: 35924.9297\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 39938.9766 - val_loss: 35572.3242\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 38773.9766 - val_loss: 35257.3203\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 39120.5312 - val_loss: 34982.4609\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 38662.2969 - val_loss: 34749.0586\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 37582.7188 - val_loss: 34558.0273\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 35699.2656 - val_loss: 34408.3008\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 34615.8398 - val_loss: 34298.1562\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 34864.1641 - val_loss: 34231.3086\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 33108.5195 - val_loss: 34203.0430\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 33089.1797 - val_loss: 34212.6797\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 31299.4043 - val_loss: 34256.7188\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 31621.2539 - val_loss: 34337.7109\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 30994.4707 - val_loss: 34443.5391\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 30668.1719 - val_loss: 34580.2031\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 29154.3965 - val_loss: 34738.9961\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 29940.2695 - val_loss: 34929.8203\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 28613.7754 - val_loss: 35150.8516\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 28459.1211 - val_loss: 35389.4258\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 27558.0195 - val_loss: 35640.3789\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 27636.8789 - val_loss: 35912.5664\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 26286.3496 - val_loss: 36204.1289\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 27241.5684 - val_loss: 36515.1328\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 25304.1660 - val_loss: 36836.3711\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 25455.3613 - val_loss: 37151.9844\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 24160.7871 - val_loss: 37428.5781\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 24334.6289 - val_loss: 37701.9609\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 24612.8008 - val_loss: 37950.7500\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 24614.9336 - val_loss: 38175.0859\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 23611.4219 - val_loss: 38404.4727\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 23669.6074 - val_loss: 38593.5703\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 22801.4629 - val_loss: 38743.5352\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 22821.5801 - val_loss: 38857.2422\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 22258.1094 - val_loss: 38963.9062\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 21924.0391 - val_loss: 39016.8516\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 21576.2676 - val_loss: 39073.9297\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 21140.3477 - val_loss: 39093.4727\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 21516.1230 - val_loss: 39113.0938\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 21003.4648 - val_loss: 39075.5664\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 21395.3574 - val_loss: 38999.5898\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 20241.2559 - val_loss: 38934.4219\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 20409.9316 - val_loss: 38895.6641\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 20660.0859 - val_loss: 38831.8008\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 19609.9746 - val_loss: 38695.0000\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 19234.0293 - val_loss: 38581.2500\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 19053.7363 - val_loss: 38442.7578\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 19452.8887 - val_loss: 38266.6719\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 18326.8086 - val_loss: 38120.4922\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 17802.9961 - val_loss: 37986.6641\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 18843.6719 - val_loss: 37777.8516\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 18395.9844 - val_loss: 37514.1328\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 18866.0488 - val_loss: 37245.0820\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 17131.7910 - val_loss: 37002.6094\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 17255.1152 - val_loss: 36764.1797\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 17671.5488 - val_loss: 36551.9453\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 16754.7344 - val_loss: 36359.7461\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 17629.5742 - val_loss: 36223.6641\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 17326.1055 - val_loss: 36132.6289\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 16476.4707 - val_loss: 36047.3398\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 16717.4316 - val_loss: 35964.0117\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 16247.9189 - val_loss: 35874.4922\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 16980.2793 - val_loss: 35785.7500\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 17106.2891 - val_loss: 35742.2227\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 16270.3545 - val_loss: 35688.6094\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 16661.8789 - val_loss: 35585.2617\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 15999.3447 - val_loss: 35491.4492\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 16743.0117 - val_loss: 35328.8594\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 16144.0479 - val_loss: 35205.4062\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 16413.9512 - val_loss: 35085.2812\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 15569.4199 - val_loss: 34990.8438\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 16555.5449 - val_loss: 34842.8867\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 15345.6055 - val_loss: 34691.5039\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 15633.5869 - val_loss: 34537.8906\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 15518.7529 - val_loss: 34430.2383\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 14856.4375 - val_loss: 34343.0703\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 14999.0107 - val_loss: 34281.4023\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 14755.9365 - val_loss: 34219.4492\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 14896.6699 - val_loss: 34184.5859\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 15355.1738 - val_loss: 34106.3320\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 14933.6602 - val_loss: 33997.6094\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 15611.8271 - val_loss: 33876.3438\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 14895.5605 - val_loss: 33758.0312\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 14817.1982 - val_loss: 33615.1719\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 15206.3037 - val_loss: 33505.2031\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 14524.0557 - val_loss: 33378.8672\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 13989.5762 - val_loss: 33259.7031\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 14414.3027 - val_loss: 33164.8828\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 14368.1865 - val_loss: 33073.6211\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 14638.1006 - val_loss: 32975.2031\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 13626.2842 - val_loss: 32864.9297\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 15223.5742 - val_loss: 32690.8594\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 14555.6943 - val_loss: 32477.0215\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 14184.9697 - val_loss: 32303.0508\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 14746.1650 - val_loss: 32209.5742\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 14384.1123 - val_loss: 32126.9648\n",
            "Training MSE: 14384.1123046875, Test MSE: 32126.96484375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Bidirectional LSTM (Heavy Model)"
      ],
      "metadata": {
        "id": "EITXR11VctmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, Bidirectional, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Columns to train on\n",
        "columns_to_train = [\"week\", \"checkout_price\", \"base_price\", \"emailer_for_promotion\", \"homepage_featured\", \"Day\", \"Month\", \"Year\", \"Quarter\"]\n",
        "\n",
        "# Extract features and target variable\n",
        "X = lstm_data[columns_to_train].values\n",
        "y = lstm_data[\"num_orders\"].values\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LSTM input (assuming a time series structure)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Model\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(128, activation='relu', return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(64, activation='relu')))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compilation\n",
        "optimizer = Adam(learning_rate=0.0005)\n",
        "model.compile(loss='mse', optimizer=optimizer)\n",
        "\n",
        "# Training with early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(x=X_train, y=y_train, epochs=300, batch_size=512, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Calculate training MSE manually\n",
        "train_preds = model.predict(X_train)\n",
        "train_mse = mean_squared_error(y_train, train_preds)\n",
        "print(f'Manually Calculated Training MSE: {train_mse}')\n",
        "\n",
        "# Print MSE\n",
        "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Training MSE: {history.history[\"loss\"][-1]}, Test MSE: {test_loss}')\n"
      ],
      "metadata": {
        "id": "5qJwlokvdGbO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c2b7222-0665-4d96-95ac-201e84756a3b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1/1 [==============================] - 7s 7s/step - loss: 82286.2812 - val_loss: 67144.2969\n",
            "Epoch 2/300\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 82235.2500 - val_loss: 67141.7266\n",
            "Epoch 3/300\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 82188.3984 - val_loss: 67139.1875\n",
            "Epoch 4/300\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 82143.4219 - val_loss: 67136.6562\n",
            "Epoch 5/300\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 82098.6484 - val_loss: 67134.1250\n",
            "Epoch 6/300\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 82053.8047 - val_loss: 67131.5703\n",
            "Epoch 7/300\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 82009.6484 - val_loss: 67128.9766\n",
            "Epoch 8/300\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 81964.8438 - val_loss: 67126.3359\n",
            "Epoch 9/300\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 81917.2578 - val_loss: 67123.6562\n",
            "Epoch 10/300\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 81866.5625 - val_loss: 67120.9297\n",
            "Epoch 11/300\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 81814.0625 - val_loss: 67118.0781\n",
            "Epoch 12/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 81757.6641 - val_loss: 67115.1406\n",
            "Epoch 13/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 81697.3672 - val_loss: 67112.1172\n",
            "Epoch 14/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 81633.1094 - val_loss: 67108.9531\n",
            "Epoch 15/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 81564.5078 - val_loss: 67105.6562\n",
            "Epoch 16/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 81491.0859 - val_loss: 67102.2031\n",
            "Epoch 17/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 81412.4688 - val_loss: 67098.6016\n",
            "Epoch 18/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 81328.3984 - val_loss: 67094.8281\n",
            "Epoch 19/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 81238.4844 - val_loss: 67090.8828\n",
            "Epoch 20/300\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 81142.2266 - val_loss: 67086.7188\n",
            "Epoch 21/300\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 81039.3281 - val_loss: 67082.3438\n",
            "Epoch 22/300\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 80929.7188 - val_loss: 67077.7422\n",
            "Epoch 23/300\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 80812.8047 - val_loss: 67072.8906\n",
            "Epoch 24/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 80688.0859 - val_loss: 67067.7656\n",
            "Epoch 25/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 80555.2656 - val_loss: 67062.3516\n",
            "Epoch 26/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 80413.1875 - val_loss: 67056.6172\n",
            "Epoch 27/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 80261.3281 - val_loss: 67050.5391\n",
            "Epoch 28/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 80099.9141 - val_loss: 67044.0703\n",
            "Epoch 29/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 79928.2422 - val_loss: 67037.1797\n",
            "Epoch 30/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 79745.4453 - val_loss: 67029.8438\n",
            "Epoch 31/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 79551.4766 - val_loss: 67022.0312\n",
            "Epoch 32/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 79345.6562 - val_loss: 67013.6719\n",
            "Epoch 33/300\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 79127.5625 - val_loss: 67004.7344\n",
            "Epoch 34/300\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 78897.0000 - val_loss: 66995.1719\n",
            "Epoch 35/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 78653.4297 - val_loss: 66984.9297\n",
            "Epoch 36/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 78396.3828 - val_loss: 66973.9531\n",
            "Epoch 37/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 78125.7422 - val_loss: 66962.1562\n",
            "Epoch 38/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 77840.7500 - val_loss: 66949.4688\n",
            "Epoch 39/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 77540.6328 - val_loss: 66935.8047\n",
            "Epoch 40/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 77225.5000 - val_loss: 66921.0625\n",
            "Epoch 41/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 76894.5000 - val_loss: 66905.2188\n",
            "Epoch 42/300\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 76547.0859 - val_loss: 66888.1641\n",
            "Epoch 43/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 76182.5547 - val_loss: 66869.7812\n",
            "Epoch 44/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 75799.9141 - val_loss: 66850.0000\n",
            "Epoch 45/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 75398.0547 - val_loss: 66828.6875\n",
            "Epoch 46/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 74976.5234 - val_loss: 66805.7266\n",
            "Epoch 47/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 74534.9844 - val_loss: 66781.0156\n",
            "Epoch 48/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 74073.0703 - val_loss: 66754.3672\n",
            "Epoch 49/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 73590.2656 - val_loss: 66725.6562\n",
            "Epoch 50/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 73086.5859 - val_loss: 66694.6328\n",
            "Epoch 51/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 72561.6562 - val_loss: 66661.2188\n",
            "Epoch 52/300\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 72014.9219 - val_loss: 66625.2344\n",
            "Epoch 53/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 71445.8906 - val_loss: 66586.5000\n",
            "Epoch 54/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 70855.3203 - val_loss: 66544.7734\n",
            "Epoch 55/300\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 70243.2188 - val_loss: 66499.9297\n",
            "Epoch 56/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 69608.8203 - val_loss: 66451.7734\n",
            "Epoch 57/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 68952.7031 - val_loss: 66400.0938\n",
            "Epoch 58/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 68274.7969 - val_loss: 66344.6797\n",
            "Epoch 59/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 67573.3203 - val_loss: 66285.3438\n",
            "Epoch 60/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 66849.5547 - val_loss: 66221.8672\n",
            "Epoch 61/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 66103.7734 - val_loss: 66154.0547\n",
            "Epoch 62/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 65335.7539 - val_loss: 66081.6641\n",
            "Epoch 63/300\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 64545.7227 - val_loss: 66004.4844\n",
            "Epoch 64/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 63734.2461 - val_loss: 65922.2812\n",
            "Epoch 65/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 62901.6797 - val_loss: 65834.8359\n",
            "Epoch 66/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 62048.6719 - val_loss: 65741.9688\n",
            "Epoch 67/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 61176.1992 - val_loss: 65643.4766\n",
            "Epoch 68/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 60284.6836 - val_loss: 65539.0000\n",
            "Epoch 69/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 59375.2500 - val_loss: 65428.3359\n",
            "Epoch 70/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 58448.7578 - val_loss: 65311.4844\n",
            "Epoch 71/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 57505.9062 - val_loss: 65188.2891\n",
            "Epoch 72/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 56547.4766 - val_loss: 65058.6016\n",
            "Epoch 73/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 55573.3164 - val_loss: 64922.2500\n",
            "Epoch 74/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 54584.7461 - val_loss: 64779.1484\n",
            "Epoch 75/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 53583.9648 - val_loss: 64629.2461\n",
            "Epoch 76/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 52571.5352 - val_loss: 64472.4844\n",
            "Epoch 77/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 51548.7070 - val_loss: 64308.8086\n",
            "Epoch 78/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 50517.0508 - val_loss: 64138.2148\n",
            "Epoch 79/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 49478.6992 - val_loss: 63960.7227\n",
            "Epoch 80/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 48435.8086 - val_loss: 63776.4062\n",
            "Epoch 81/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 47391.0312 - val_loss: 63585.3750\n",
            "Epoch 82/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 46346.4844 - val_loss: 63387.7422\n",
            "Epoch 83/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 45304.0781 - val_loss: 63183.6367\n",
            "Epoch 84/300\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 44265.8633 - val_loss: 62973.2969\n",
            "Epoch 85/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 43233.7227 - val_loss: 62756.9727\n",
            "Epoch 86/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 42209.9062 - val_loss: 62534.9961\n",
            "Epoch 87/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 41196.3984 - val_loss: 62307.7109\n",
            "Epoch 88/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 40194.8477 - val_loss: 62075.4922\n",
            "Epoch 89/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 39206.9922 - val_loss: 61838.7930\n",
            "Epoch 90/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 38234.7070 - val_loss: 61598.0898\n",
            "Epoch 91/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 37279.4961 - val_loss: 61353.8984\n",
            "Epoch 92/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 36342.8789 - val_loss: 61106.6914\n",
            "Epoch 93/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 35426.3008 - val_loss: 60857.0039\n",
            "Epoch 94/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 34530.9219 - val_loss: 60605.3867\n",
            "Epoch 95/300\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 33657.6680 - val_loss: 60352.5039\n",
            "Epoch 96/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 32807.4336 - val_loss: 60098.9844\n",
            "Epoch 97/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 31981.1211 - val_loss: 59845.4609\n",
            "Epoch 98/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 31179.6016 - val_loss: 59592.5273\n",
            "Epoch 99/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 30402.8008 - val_loss: 59340.7852\n",
            "Epoch 100/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 29650.5273 - val_loss: 59090.9258\n",
            "Epoch 101/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 28922.6934 - val_loss: 58843.4453\n",
            "Epoch 102/300\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 28219.0332 - val_loss: 58598.7070\n",
            "Epoch 103/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 27538.2207 - val_loss: 58357.5156\n",
            "Epoch 104/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 26880.1523 - val_loss: 58120.2930\n",
            "Epoch 105/300\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 26243.6602 - val_loss: 57887.4570\n",
            "Epoch 106/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 25627.5410 - val_loss: 57659.3438\n",
            "Epoch 107/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 25030.6094 - val_loss: 57436.3359\n",
            "Epoch 108/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 24451.4570 - val_loss: 57218.6211\n",
            "Epoch 109/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 23888.7012 - val_loss: 57006.4727\n",
            "Epoch 110/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 23341.1914 - val_loss: 56800.0508\n",
            "Epoch 111/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 22807.3203 - val_loss: 56599.4844\n",
            "Epoch 112/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 22285.8262 - val_loss: 56404.6719\n",
            "Epoch 113/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 21775.5293 - val_loss: 56215.6484\n",
            "Epoch 114/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 21275.3691 - val_loss: 56032.3594\n",
            "Epoch 115/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 20784.4785 - val_loss: 55854.5508\n",
            "Epoch 116/300\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 20302.0703 - val_loss: 55682.0156\n",
            "Epoch 117/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 19827.4316 - val_loss: 55514.4648\n",
            "Epoch 118/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 19360.0898 - val_loss: 55351.5859\n",
            "Epoch 119/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 18899.6602 - val_loss: 55193.0156\n",
            "Epoch 120/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 18446.0430 - val_loss: 55038.3828\n",
            "Epoch 121/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 17999.3281 - val_loss: 54887.2148\n",
            "Epoch 122/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 17559.3574 - val_loss: 54739.0859\n",
            "Epoch 123/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 17126.2617 - val_loss: 54593.4727\n",
            "Epoch 124/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 16700.7207 - val_loss: 54449.6367\n",
            "Epoch 125/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 16283.0742 - val_loss: 54307.0781\n",
            "Epoch 126/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 15873.2959 - val_loss: 54165.1562\n",
            "Epoch 127/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 15472.0498 - val_loss: 54023.1953\n",
            "Epoch 128/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 15080.0068 - val_loss: 53880.5430\n",
            "Epoch 129/300\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 14697.7598 - val_loss: 53736.6719\n",
            "Epoch 130/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 14325.4990 - val_loss: 53591.1328\n",
            "Epoch 131/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 13964.0127 - val_loss: 53443.6328\n",
            "Epoch 132/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 13613.3789 - val_loss: 53293.6172\n",
            "Epoch 133/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 13274.0068 - val_loss: 53140.8242\n",
            "Epoch 134/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 12946.3428 - val_loss: 52984.8867\n",
            "Epoch 135/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 12630.2891 - val_loss: 52825.4492\n",
            "Epoch 136/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 12325.5605 - val_loss: 52662.2969\n",
            "Epoch 137/300\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 12032.2588 - val_loss: 52495.1641\n",
            "Epoch 138/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 11750.3193 - val_loss: 52324.0117\n",
            "Epoch 139/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 11479.8047 - val_loss: 52148.8828\n",
            "Epoch 140/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 11220.9414 - val_loss: 51969.7891\n",
            "Epoch 141/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 10973.3145 - val_loss: 51786.7070\n",
            "Epoch 142/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 10736.7412 - val_loss: 51599.7578\n",
            "Epoch 143/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 10511.1221 - val_loss: 51409.1641\n",
            "Epoch 144/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 10296.2510 - val_loss: 51215.0078\n",
            "Epoch 145/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 10091.5518 - val_loss: 51017.5156\n",
            "Epoch 146/300\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 9897.1133 - val_loss: 50816.8711\n",
            "Epoch 147/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 9712.1455 - val_loss: 50613.5469\n",
            "Epoch 148/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 9536.3018 - val_loss: 50408.3906\n",
            "Epoch 149/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 9369.0752 - val_loss: 50201.6133\n",
            "Epoch 150/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 9210.3994 - val_loss: 49993.5273\n",
            "Epoch 151/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 9059.6836 - val_loss: 49784.4648\n",
            "Epoch 152/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 8916.4219 - val_loss: 49575.1914\n",
            "Epoch 153/300\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 8780.0137 - val_loss: 49366.3867\n",
            "Epoch 154/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 8650.1182 - val_loss: 49158.3711\n",
            "Epoch 155/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 8526.4570 - val_loss: 48951.8008\n",
            "Epoch 156/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 8408.3984 - val_loss: 48746.7812\n",
            "Epoch 157/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 8295.6855 - val_loss: 48543.7656\n",
            "Epoch 158/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 8187.7769 - val_loss: 48343.0859\n",
            "Epoch 159/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 8084.3633 - val_loss: 48144.9922\n",
            "Epoch 160/300\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 7984.9878 - val_loss: 47949.9570\n",
            "Epoch 161/300\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 7889.2876 - val_loss: 47757.9688\n",
            "Epoch 162/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 7796.9722 - val_loss: 47569.2422\n",
            "Epoch 163/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 7707.7959 - val_loss: 47383.7344\n",
            "Epoch 164/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 7621.2959 - val_loss: 47201.4531\n",
            "Epoch 165/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 7537.1055 - val_loss: 47022.3789\n",
            "Epoch 166/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 7455.1289 - val_loss: 46846.6016\n",
            "Epoch 167/300\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 7374.7490 - val_loss: 46674.1133\n",
            "Epoch 168/300\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 7295.8481 - val_loss: 46505.4492\n",
            "Epoch 169/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 7218.6001 - val_loss: 46340.1523\n",
            "Epoch 170/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 7143.9990 - val_loss: 46177.4727\n",
            "Epoch 171/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 7070.8477 - val_loss: 46017.2734\n",
            "Epoch 172/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 6998.6582 - val_loss: 45859.2383\n",
            "Epoch 173/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 6927.1440 - val_loss: 45703.0156\n",
            "Epoch 174/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6856.2061 - val_loss: 45548.4492\n",
            "Epoch 175/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6786.0449 - val_loss: 45395.4492\n",
            "Epoch 176/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 6716.7812 - val_loss: 45243.8203\n",
            "Epoch 177/300\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 6649.0454 - val_loss: 45092.8867\n",
            "Epoch 178/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6582.5767 - val_loss: 44942.6797\n",
            "Epoch 179/300\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 6517.8477 - val_loss: 44793.0781\n",
            "Epoch 180/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6454.4790 - val_loss: 44643.8789\n",
            "Epoch 181/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6392.2490 - val_loss: 44494.9219\n",
            "Epoch 182/300\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 6331.3247 - val_loss: 44345.6992\n",
            "Epoch 183/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6271.5991 - val_loss: 44195.9688\n",
            "Epoch 184/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 6213.1084 - val_loss: 44045.6328\n",
            "Epoch 185/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 6155.7998 - val_loss: 43894.8359\n",
            "Epoch 186/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6099.7812 - val_loss: 43743.7773\n",
            "Epoch 187/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 6044.8296 - val_loss: 43592.0703\n",
            "Epoch 188/300\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 5991.1924 - val_loss: 43439.7539\n",
            "Epoch 189/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 5939.4282 - val_loss: 43286.3789\n",
            "Epoch 190/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 5888.4097 - val_loss: 43132.8203\n",
            "Epoch 191/300\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 5838.0166 - val_loss: 42978.9453\n",
            "Epoch 192/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 5788.8228 - val_loss: 42824.5078\n",
            "Epoch 193/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 5741.2817 - val_loss: 42669.0703\n",
            "Epoch 194/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5695.1870 - val_loss: 42512.4844\n",
            "Epoch 195/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 5650.9180 - val_loss: 42355.4414\n",
            "Epoch 196/300\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 5607.7583 - val_loss: 42195.1133\n",
            "Epoch 197/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 5565.9653 - val_loss: 42031.0000\n",
            "Epoch 198/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 5524.6582 - val_loss: 41863.6953\n",
            "Epoch 199/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 5483.9429 - val_loss: 41693.6328\n",
            "Epoch 200/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5444.1997 - val_loss: 41521.8203\n",
            "Epoch 201/300\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 5405.9629 - val_loss: 41349.3281\n",
            "Epoch 202/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 5368.8105 - val_loss: 41176.5156\n",
            "Epoch 203/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 5332.9014 - val_loss: 41003.0703\n",
            "Epoch 204/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5298.2485 - val_loss: 40828.8203\n",
            "Epoch 205/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 5264.8843 - val_loss: 40654.1797\n",
            "Epoch 206/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 5232.9058 - val_loss: 40479.3281\n",
            "Epoch 207/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 5202.1499 - val_loss: 40304.2773\n",
            "Epoch 208/300\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 5172.7124 - val_loss: 40129.3203\n",
            "Epoch 209/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5144.4321 - val_loss: 39954.5977\n",
            "Epoch 210/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 5117.2241 - val_loss: 39779.6250\n",
            "Epoch 211/300\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 5090.9468 - val_loss: 39604.1797\n",
            "Epoch 212/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5065.4741 - val_loss: 39428.2930\n",
            "Epoch 213/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 5040.7974 - val_loss: 39252.0898\n",
            "Epoch 214/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5016.8066 - val_loss: 39075.9258\n",
            "Epoch 215/300\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 4993.6997 - val_loss: 38899.1992\n",
            "Epoch 216/300\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 4971.4746 - val_loss: 38721.5156\n",
            "Epoch 217/300\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 4950.4111 - val_loss: 38542.1836\n",
            "Epoch 218/300\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 4929.8857 - val_loss: 38361.6094\n",
            "Epoch 219/300\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 4909.9595 - val_loss: 38180.3477\n",
            "Epoch 220/300\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 4890.4717 - val_loss: 37998.9180\n",
            "Epoch 221/300\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 4871.3511 - val_loss: 37817.4180\n",
            "Epoch 222/300\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 4852.6636 - val_loss: 37636.4883\n",
            "Epoch 223/300\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 4834.5376 - val_loss: 37456.9648\n",
            "Epoch 224/300\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 4816.8652 - val_loss: 37278.9766\n",
            "Epoch 225/300\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 4799.2949 - val_loss: 37102.6328\n",
            "Epoch 226/300\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 4781.9541 - val_loss: 36927.6680\n",
            "Epoch 227/300\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 4764.8599 - val_loss: 36754.2070\n",
            "Epoch 228/300\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 4748.0151 - val_loss: 36582.2344\n",
            "Epoch 229/300\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 4731.4375 - val_loss: 36412.2188\n",
            "Epoch 230/300\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 4714.9448 - val_loss: 36244.2734\n",
            "Epoch 231/300\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 4698.4810 - val_loss: 36078.2148\n",
            "Epoch 232/300\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 4682.1436 - val_loss: 35913.1562\n",
            "Epoch 233/300\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 4665.9780 - val_loss: 35749.0352\n",
            "Epoch 234/300\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 4649.9531 - val_loss: 35586.0039\n",
            "Epoch 235/300\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 4634.0449 - val_loss: 35423.9297\n",
            "Epoch 236/300\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 4618.1211 - val_loss: 35262.7070\n",
            "Epoch 237/300\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 4602.3486 - val_loss: 35101.8242\n",
            "Epoch 238/300\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 4586.6650 - val_loss: 34941.1641\n",
            "Epoch 239/300\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 4570.9761 - val_loss: 34780.9102\n",
            "Epoch 240/300\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 4555.2798 - val_loss: 34621.2148\n",
            "Epoch 241/300\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 4539.5967 - val_loss: 34462.3438\n",
            "Epoch 242/300\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 4523.9077 - val_loss: 34304.3008\n",
            "Epoch 243/300\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 4508.2573 - val_loss: 34147.0664\n",
            "Epoch 244/300\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 4492.4961 - val_loss: 33990.3750\n",
            "Epoch 245/300\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 4476.4551 - val_loss: 33834.3633\n",
            "Epoch 246/300\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 4460.1675 - val_loss: 33678.7188\n",
            "Epoch 247/300\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 4443.6313 - val_loss: 33523.7148\n",
            "Epoch 248/300\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 4426.8955 - val_loss: 33369.1523\n",
            "Epoch 249/300\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 4409.9790 - val_loss: 33215.0938\n",
            "Epoch 250/300\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 4393.0146 - val_loss: 33061.5273\n",
            "Epoch 251/300\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 4376.1875 - val_loss: 32908.4844\n",
            "Epoch 252/300\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 4359.5249 - val_loss: 32755.9453\n",
            "Epoch 253/300\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 4343.1870 - val_loss: 32603.4590\n",
            "Epoch 254/300\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 4327.2378 - val_loss: 32450.6504\n",
            "Epoch 255/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 4311.6763 - val_loss: 32297.6211\n",
            "Epoch 256/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 4296.4258 - val_loss: 32144.5469\n",
            "Epoch 257/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 4281.4316 - val_loss: 31991.7207\n",
            "Epoch 258/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 4266.5190 - val_loss: 31838.8066\n",
            "Epoch 259/300\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 4251.6904 - val_loss: 31685.8730\n",
            "Epoch 260/300\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 4236.8989 - val_loss: 31532.8828\n",
            "Epoch 261/300\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 4222.0190 - val_loss: 31379.8965\n",
            "Epoch 262/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 4206.9370 - val_loss: 31226.8457\n",
            "Epoch 263/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 4191.8223 - val_loss: 31073.9648\n",
            "Epoch 264/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 4176.4141 - val_loss: 30921.6660\n",
            "Epoch 265/300\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 4160.7148 - val_loss: 30769.6895\n",
            "Epoch 266/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 4145.1519 - val_loss: 30617.8926\n",
            "Epoch 267/300\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 4129.7202 - val_loss: 30466.4531\n",
            "Epoch 268/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 4114.1709 - val_loss: 30314.9648\n",
            "Epoch 269/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 4098.5430 - val_loss: 30163.2109\n",
            "Epoch 270/300\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 4082.5615 - val_loss: 30011.9004\n",
            "Epoch 271/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 4066.4375 - val_loss: 29860.8379\n",
            "Epoch 272/300\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 4050.2134 - val_loss: 29709.4180\n",
            "Epoch 273/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 4033.9031 - val_loss: 29557.5840\n",
            "Epoch 274/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 4017.7346 - val_loss: 29404.9551\n",
            "Epoch 275/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 4001.5825 - val_loss: 29250.9375\n",
            "Epoch 276/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 3985.5320 - val_loss: 29096.2539\n",
            "Epoch 277/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 3969.4807 - val_loss: 28942.0156\n",
            "Epoch 278/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 3953.2207 - val_loss: 28788.4863\n",
            "Epoch 279/300\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 3937.0776 - val_loss: 28635.9492\n",
            "Epoch 280/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 3920.8838 - val_loss: 28484.0957\n",
            "Epoch 281/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 3904.7922 - val_loss: 28331.7773\n",
            "Epoch 282/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 3888.8013 - val_loss: 28181.4180\n",
            "Epoch 283/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 3872.9036 - val_loss: 28034.1953\n",
            "Epoch 284/300\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 3856.8596 - val_loss: 27887.6680\n",
            "Epoch 285/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 3840.9155 - val_loss: 27741.1504\n",
            "Epoch 286/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 3824.9470 - val_loss: 27594.8770\n",
            "Epoch 287/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 3808.8923 - val_loss: 27448.0020\n",
            "Epoch 288/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 3792.8171 - val_loss: 27301.3105\n",
            "Epoch 289/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 3776.7844 - val_loss: 27154.4102\n",
            "Epoch 290/300\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 3760.7188 - val_loss: 27007.4375\n",
            "Epoch 291/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 3744.6736 - val_loss: 26860.5781\n",
            "Epoch 292/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 3728.7000 - val_loss: 26714.6523\n",
            "Epoch 293/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 3712.7542 - val_loss: 26569.2676\n",
            "Epoch 294/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 3696.7375 - val_loss: 26425.0469\n",
            "Epoch 295/300\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 3680.6589 - val_loss: 26281.3574\n",
            "Epoch 296/300\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 3664.5522 - val_loss: 26138.5293\n",
            "Epoch 297/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 3648.4802 - val_loss: 25996.6289\n",
            "Epoch 298/300\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 3632.3484 - val_loss: 25854.2949\n",
            "Epoch 299/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 3616.0588 - val_loss: 25712.8574\n",
            "Epoch 300/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 3599.7163 - val_loss: 25572.2246\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "Manually Calculated Training MSE: 35996.25525912163\n",
            "Training MSE: 3599.71630859375, Test MSE: 25572.224609375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVNMngwKekly"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ep4cNEarekcd"
      },
      "execution_count": 19,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}